{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OzgzY0D10Vqr"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, LeakyReLU, Flatten\n",
    "from tensorflow.keras.layers import Dense, Conv2DTranspose, Reshape, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6YBOUan30gQ1"
   },
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "  model = Sequential()\n",
    "  model.add(Dense(4*4*512, input_dim = 128+54))\n",
    "  model.add(ReLU())\n",
    "  \n",
    "  model.add(Reshape((4,4,512)))\n",
    "  model.add(Conv2DTranspose(256, (5, 5), strides=(2,2), padding='same'))\n",
    "  model.add(ReLU())\n",
    "            \n",
    "  model.add(Conv2DTranspose(128, (5, 5), strides=(2,2), padding='same'))\n",
    "  model.add(ReLU())\n",
    "  \n",
    "  model.add(Conv2DTranspose(64, (5, 5), strides=(2,2), padding='same'))\n",
    "  model.add(ReLU())\n",
    "  \n",
    "  model.add(Conv2DTranspose(1, (5, 5), strides=(2,2), padding='same', activation='sigmoid'))\n",
    "            \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OAdTMfdP0k0b"
   },
   "outputs": [],
   "source": [
    "def make_discriminator():\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=(64,64,1)))\n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  \n",
    "  model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  \n",
    "  model.add(Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "  model.add(Conv2D(512, (5, 5), strides=(2, 2), padding='same'))\n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  \n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(1))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdVpZBw40lNV"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gen_fake_image(z):\n",
    "    image_sample = generator(z)\n",
    "    return tf.cast(image_sample, dtype = tf.dtypes.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fPGBOcjKdDqF"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def cal_gradient_penalty(real_data, z):\n",
    "  fake_data = gen_fake_image(z)\n",
    "\n",
    "  alpha = tf.random.uniform(\n",
    "        shape=[BATCH_SIZE,1], \n",
    "        minval=0.,\n",
    "        maxval=1.\n",
    "    )\n",
    "\n",
    "  differences = fake_data - real_data\n",
    "  interpolates = real_data + tf.reshape((alpha*tf.reshape(differences, (BATCH_SIZE,-1))), (BATCH_SIZE, 64, 64, 1))\n",
    "  gradients = tf.gradients(discriminator(interpolates), [interpolates])[0]\n",
    "  slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis = 1))\n",
    "  gradient_penalty = tf.reduce_mean((slopes-1.)**2)\n",
    "  return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_uZe_pNGXuj"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def cal_disc_cost(real_data, z, gradient_penalty):\n",
    "  fake_data = gen_fake_image(z)\n",
    "  return tf.reduce_mean(discriminator(fake_data)) - tf.reduce_mean(discriminator(real_data)) + LAMBDA*gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KGGIiKMF-xEi"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def make_one_hot(c, num, seed = None, num_char = 54):\n",
    "  return tf.concat([tf.reshape(tf.tile(tf.one_hot(c, num_char, dtype = tf.dtypes.float32), [num]), [-1, num_char]),\n",
    "                    tf.random.normal(shape = (num,128), dtype = tf.dtypes.float32, seed = seed)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WnXVZcUKbf4L"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def make_consist(num_char = 54):\n",
    "  z = tf.tile(tf.random.normal(shape = (1,128), dtype = tf.dtypes.float32), [num_char,1])\n",
    "  letter_one_hot = tf.reshape(tf.one_hot(0, num_char, dtype = tf.dtypes.float32), [1,-1])\n",
    "  for c in range(1, num_char):\n",
    "    temp = tf.reshape(tf.one_hot(c, num_char, dtype = tf.dtypes.float32), [1, -1])\n",
    "    letter_one_hot = tf.concat([letter_one_hot, temp], axis = 0)\n",
    "  return tf.concat([letter_one_hot,z], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E9B5_q37fd6Y"
   },
   "outputs": [],
   "source": [
    "def update_disc(batch_data, c):\n",
    "  real_data = tf.cast(batch_data, dtype = tf.float32)\n",
    "  tf.random.set_seed(None)\n",
    "  z = make_one_hot(c, BATCH_SIZE)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    gradient_penalty = cal_gradient_penalty(real_data, z)\n",
    "    disc_cost = cal_disc_cost(real_data, z, gradient_penalty)\n",
    "  disc_gradient = tape.gradient(disc_cost, discriminator.trainable_variables)\n",
    "  disc_opt.apply_gradients(zip(disc_gradient, discriminator.trainable_variables))\n",
    "  return disc_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TmczSgI-0z7x"
   },
   "outputs": [],
   "source": [
    "def update_gen(c):\n",
    "    tf.random.set_seed(None)\n",
    "    z = make_one_hot(c, BATCH_SIZE)\n",
    "    with tf.GradientTape() as tape:\n",
    "      fake_data = gen_fake_image(z)\n",
    "      gen_cost = -tf.reduce_mean(discriminator(fake_data))\n",
    "    gen_gradient = tape.gradient(gen_cost, generator.trainable_variables)\n",
    "    gen_opt.apply_gradients(zip(gen_gradient, generator.trainable_variables))\n",
    "    return gen_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pRx-fP5v04mN"
   },
   "outputs": [],
   "source": [
    "def plot_image(iteration, numchar = 54):\n",
    "    z = make_consist(num_char = numchar)\n",
    "    fig = plt.figure(figsize=(9,6))\n",
    "    image_sample = gen_fake_image(z)\n",
    "    image_sample = image_sample.numpy().reshape(-1,64,64,1)\n",
    "    for i in range(image_sample.shape[0]):\n",
    "      plt.subplot(9, 6, i+1)\n",
    "      plt.axis('off')\n",
    "      plt.imshow(image_sample[i, :, :, 0], cmap = 'gray')\n",
    "      plt.savefig('save_img/iteration{}.png'.format(iteration))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "   \n",
    "def plot_loss(disc_cost_epoch, gen_cost_epoch):\n",
    "  fig, axes = plt.subplots(1,2, figsize = (16,4))\n",
    "  axes[0].plot(disc_cost_epoch, label = 'discriminator loss', color = 'orange')\n",
    "  axes[0].legend()\n",
    "  \n",
    "  axes[1].plot(gen_cost_epoch, label = 'generator loss', color = 'blue')\n",
    "  axes[1].legend()\n",
    "  plt.show()\n",
    "  plt.clf()\n",
    "\n",
    "\n",
    "def plot_same(z_fix, iteration):\n",
    "    fig = plt.figure(figsize=(9,6))\n",
    "    image_sample = gen_fake_image(z_fix)\n",
    "    image_sample = image_sample.numpy().reshape(-1,64,64,1)\n",
    "    for i in range(image_sample.shape[0]):\n",
    "      plt.subplot(9, 6, i+1)\n",
    "      plt.axis('off')\n",
    "      plt.imshow(image_sample[i, :, :, 0], cmap = 'gray')\n",
    "      plt.savefig('save_img_fix/iteration{}.png'.format(iteration))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONUURVbzPeV1"
   },
   "outputs": [],
   "source": [
    "disc_opt = tf.keras.optimizers.Adam(\n",
    "        learning_rate=1e-4,\n",
    "        beta_1=0.5, \n",
    "        beta_2=0.99)\n",
    "\n",
    "gen_opt = tf.keras.optimizers.Adam(\n",
    "        learning_rate=1e-4,\n",
    "        beta_1=0.5, \n",
    "        beta_2=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ASH_T1m2EQ8"
   },
   "outputs": [],
   "source": [
    "# z_fix = make_consist(num_char = 54)\n",
    "# np.save('z_fix.npy', z_fix.numpy())\n",
    "\n",
    "z_fix = tf.constant(np.load('z_fix.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msPoygB009NO"
   },
   "outputs": [],
   "source": [
    "data = np.load('fontarray.npy')\n",
    "data = data/255.0\n",
    "data = data.reshape(-1,54,64,64,1)\n",
    "data = data.transpose(1,0,2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMXTa24U1n3n"
   },
   "outputs": [],
   "source": [
    "generator = make_generator()\n",
    "discriminator = make_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#กขฃฅคฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรลวศษสหฬอฮ๐๑๒๓๔๕๖๗๘๙\n",
    "\n",
    "BATCH_SIZE = 64 # Batch size\n",
    "CRITIC_ITERS = 5 # For WGAN and WGAN-GP, number of critic iters per gen iter\n",
    "LAMBDA = 10 # Gradient penalty lambda hyperparameter\n",
    "ITERS = 2500 # How many generator iterations to train for \n",
    "OUTPUT_DIM = (64,64,1) # Number of pixels \n",
    "sample_interval = 10\n",
    "\n",
    "gen_cost_epoch = []\n",
    "disc_cost_epoch = []\n",
    "\n",
    "for iteration in range(ITERS):\n",
    "    start = time.time()\n",
    "    for c in range(54):\n",
    "      data_iter = tf.data.Dataset.from_tensor_slices(data[c]).shuffle(buffer_size = 100).batch(BATCH_SIZE, drop_remainder=True).repeat()\n",
    "      data_batch = iter(data_iter)\n",
    "      gen_cost_c = []\n",
    "      for i in range(CRITIC_ITERS):\n",
    "        disc_cost_batch = []\n",
    "        disc_cost_batch.append(update_disc(next(data_batch),c).numpy())\n",
    "        # print('finish update c of char:{}, epoch:{}'.format(c, iteration))\n",
    "      gen_cost_c.append(update_gen(c).numpy())\n",
    "#       print('finish update gen of char:{}, epoch:{}'.format(c, iteration))\n",
    "    gen_cost_epoch.append(np.mean(gen_cost_c))\n",
    "    disc_cost_epoch.append(tf.reduce_mean(disc_cost_batch))\n",
    "    print('finish epoch:{} --- time use this epoch:{}'.format(iteration, time.time() - start))\n",
    "    if iteration % sample_interval == 0:\n",
    "        generator.save('checkpoint_gen/cp_gen_{}th_epoch.h5'.format(iteration))\n",
    "        discriminator.save('checkpoint_disc/cp_disc_{}_th_epoch.h5'.format(iteration))\n",
    "        plot_image(iteration)\n",
    "        plot_same(z_fix, iteration)\n",
    "        plot_loss(disc_cost_epoch, gen_cost_epoch)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tensorflow_glyphgan.ipynb",
   "provenance": [
    {
     "file_id": "14GjvJzR8yiEuwyRY_4_Bd7snI4auleTt",
     "timestamp": 1572999729912
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
